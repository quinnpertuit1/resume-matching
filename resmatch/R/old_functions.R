#old functions

#'
#' #' Cleans html to prepare for variable creation
#' #'
#' #' @description Use regular expressions to eliminate irrelevant html syntax/formatting
#' #' @param html = raw website html
#' #' @return a list containing clean text broken down into sections based on the number of spaces
#' #' @export
#' clean_general_html <- function(html){
#'   text_list <- list()
#'   remove <- "\\||~|!|@|#|%|\\^|&|\\*|\\(|\\)|\\{|\\}|_|\\+|\\:|\\|<|>|\\?|,|;|'|\\[|\\]|="
#'   html <- gsub("<.*?>", "", html)  #remove everything between <>
#'   html <- gsub("\\{.+?\\}", "", html)
#'   html <- gsub("&#.+?;", "", html)
#'   html <- gsub("\\n|\\r|\\t", " ", html) #remove all \n \r \t
#'   html <- gsub(remove, "", html) #remove certain characters as defined in remove
#'   html <- gsub('[\\.\\/\\$\\-]', " ", html) #remove whitespace and replace with a single space
#'   html <- gsub('([a-z])([A-Z])','\\1 \\2', html) #add space between lcase and ucase letters
#'   html <- gsub('[^a-zA-Z0-9 $]', "", html)
#'   html <- gsub("([a-zA-Z])([0-9])", "\\1 \\2", html)
#'   html <- tolower(html) #make everything lowercase
#'   sects <- c(1,str_locate_all(html,'    + ')[[1]][,1],nchar(html))
#'   for(i in 1:(length(sects)-1)){
#'     text_list[i] <- substr(html, start = sects[i], stop = sects[i + 1])
#'   }
#'   return(text_list[!sapply(text_list, is.null)])
#' }
#'
#' #'
#'
#'
#' #' Creates variables used to generate scores
#' #'
#' #' @description The variables created will be used to estimate the likelihood
#' #' that the word is useful and desired
#' #' @param text_list = a list generated by separating text into sections by an excess of spaces
#' #' @return a data.frame of variables describing a specific word within a particular section
#'
#' create_variables <- function(text_list) {
#'   vowels <- 'a|e|i|o|u'
#'   consonants <- letters[!(letters %in% c('a','e','i','o','u','y'))]
#'   consonants <- paste(consonants, collapse = '|')
#'   lapply(text_list, function(clean_text){
#'     text <- gsub('\\s+', " ", clean_text)
#'     if(text != ' '){
#'       words <- strsplit(text, " ")
#'       words <- sapply(words, function(x) x[x!=""])
#'       lengths <- sapply(words, nchar)
#'       num_vowel <- sapply(words, function(x) str_count(tolower(x), vowels ))
#'       num_consonants <- sapply(words, function(x) str_count(tolower(x), consonants))
#'       pct_consonant <- as.numeric(num_consonants/lengths)
#'       key_term <- sapply(words, function(x) x %in% key_terms)
#'       sds <- rep(sd(lengths),length(lengths))
#'       num_char <- nchar(text)
#'       contains_num <- sapply(words, function(x) str_detect(x,'[a-z][0-9]|[0-9][a-z]'))
#'       find_consec_consonants <- paste0('[',consonants,']','+','[', consonants,']?')
#'       max_consec_consonants <- sapply(words, function(x){
#'         temp <- str_locate_all(x,find_consec_consonants)
#'         max_consec <- sapply(temp, function(x){
#'           ifelse((is.na(max(x[,2] - x[,1])) | length(x[,1])) == 0, 0, max(x[,2] - x[,1]))
#'         })
#'         return(max_consec + 1)
#'       })
#'       num_unique_words <- length(unique(words))
#'       num_words <- length(words)
#'       pct_unique <- num_unique_words/num_words
#'
#'       return(data.frame(words, lengths, sds, num_vowel, num_consonants, pct_consonant,
#'                         num_char,contains_num,max_consec_consonants,
#'                         num_words,num_unique_words, pct_unique, key_term))
#'     }
#'   }
#'   )
#'
#' }
#'
#'
#'
#' #' Use Random forest model to filter the text
#' #'
#' #' @description  Uses random forest model to predict which word sections
#' #' contain real and useful text. This function takes the randomForest model
#' #' previously created and scores individual words on the probability that they are
#' #' desired for storage. If the mean score of the section is less than 20% then the
#' #' section is removed. The text is broken up by setions depending on spaces separating rows.
#' #' @param dataset = a dataset produced by the create_variables function
#' #' @return a vector of words in which the corresponding section had a chance to be useful
#'
#' predictive_filter <- function(dataset) {
#'   dataset <- dataset[!sapply(dataset, is.null)]
#'   words <- sapply(dataset, function(x){
#'     wordlist <- x$words
#'     in_dictionary <- wordlist %in% dictionary
#'     pct_in_dictionary = sum(in_dictionary)/length(in_dictionary)
#'     preds <- predict(forest, x, type = 'prob')[,2]
#'     preds[is.na(preds)]<- 0
#'     if(mean(preds)>=0.2){
#'       df <- as.character(x$words)
#'     }
#'     else {
#'       df <- NA
#'     }
#'     # use predictive filter to select words, then use dictionary to extract words not grabbed by the model
#'     #
#'     #   return(df)
#'     #   # x$preds <- preds[,2]
#'     #   # print(x)
#'     #   # print(mean(preds[,2]))
#'     #   # if(plot_data == TRUE){
#'     #   #   print(ggplot(x) + geom_density(aes(x = preds)) +xlim(0,1))
#'     #   #   readline('press enter for next chart')
#'     #   # }
#'     #   # return(x)
#'   })
#'   words <- unlist(words)
#'   words <- words[!is.na(words)]
#'   return(words)
#' }
#'
#'
#' #' Take raw html and output the clean relevant text.
#' #'
#' #' @description this function uses a combination of functions:
#' #' clean_general_html, create_variables, predictive_filter, extract_real_words, and
#' #' extract key terms. It does so in order to generate clean, useful text from any website
#' #' @param html = a websites raw html
#' #' @return clean relevant text
#' #' @examples
#' #' extract_useful_text(some_html_string)
#'
#' extract_useful_text <- function(html){
#'
#'   clean_text <- clean_general_html(html)
#'   data <- create_variables(clean_text)
#'   filter_words <- predictive_filter(data)
#'   dict_words <- extract_real_words(html)
#'
#'   #don't want words appended at the end that appear in the first section
#'   dict_words <- dict_words[!(dict_words %in% filter_words)]
#'
#'   #creates variable containing the useful text with predictive filter words
#'   # followed by the words found in the dictionary
#'   output <- c(filter_words,"NOW STARTING DICT", dict_words)
#'
#'   # stopwords are removed from the output
#'   output <- output[!(output %in% stopwords)]
#'
#'   #key_terms are extracted
#'   key_terms <- extract_key_terms(output)
#'   output <- output[is.na(as.numeric(output))]
#'   output <- paste(output, collapse = ' ')
#'   return(data.frame(text = output, key_terms, stringsAsFactors = F))
#' }
#'
#'
#'
#'
#' #' URL to job data frame function
#' #'
#' #' @description This function combines all other functions to take the href of a job posting
#' #'and output all relevant information about a job
#' #' @param href_vector = a vector containing hrefs
#' #' @param lim = the max number of hrefs to visit
#' #' @param api = whether the user wants to call the api for data
#' #' @param search_term = search term for api
#' #' @param location = location for api
#' #' @param start_point = job number starting point for the api
#' #' @return a dataframe containing href, useful text and keywords. If api is used
#' #' then it returns a dataframe containing href, useful text, keywords, and also all other api info
#'
#'
#' convert_href_to_text <- function(href_vector = NULL, lim = 5, api = FALSE, search_term = "analyst", location =  "", start_point= ""){
#'
#'   if(api == TRUE){
#'     href_vector<- call_indeed_api(search_term = search_term,
#'                                   location = location,
#'                                   start_point = start_point, lim  = lim)
#'   }
#'
#'   print('Scraping Websites')
#'   #visit hrefs and extract raw text
#'   html_text_col <- NULL
#'   lim <- ifelse(lim < nrow(href_vector), lim, nrow(href_vector))
#'   href_vector <- href_vector[1:lim,]
#'   i <- 0
#'   for(href in href_vector$href){
#'     html_text <- as.character(read_html(href))
#'     i <- i + 1
#'     cat('scraping href', i)
#'     cat('\n')
#'     html_text_col <- c(html_text_col,html_text)
#'   }
#'   print('Grabbing useful text')
#'   #write to file
#'   text_df <- NULL
#'   for(html_text in html_text_col){
#'     text <- suppressWarnings(extract_useful_text(html_text))
#'     text_df <- rbind(text_df, text)
#'   }
#'
#'   print('converting api info and text into dataframe')
#'   job_info <- cbind(href_vector, text_df, stringsAsFactors = F)
#'   return(job_info)
#' }
#'
#'
#'
#' #figure out how to pull out requirements... maybe build a model to predict those?
#'
#' # match on:
#' # Require the user to exclude certain industries
#' # Require user to for salary range
#' # Require the user to specify locations/if location matters
#' # match based on common words?
#' # match on....... look into similarity metrics
#' # match on what terms are unique to that job that most other jobs do not have. ex. cassandra
#' # .... if you know how to use cassandra and it is listed on a job, that job should come before others
#'
#'
#'
#' #on an individual determine if there is an objective section
#'
#'
#' # cosSim <- function(resume_text_corpus,job_text_corpus,z){
#' #   # Term Matrix (Resume);
#' #   tf2 <- DocumentTermMatrix(resume_text_corpus) # dtm_test <- t(as.matrix(dtm))
#' #   # Weighted Term Frequency;
#' #   normTF2 <- 1 + log10(as.matrix(tf2)) # Calculate the Log normalized Values;
#' #   normTF2 <- data.frame(t(normTF2))
#' #   # Document Frequency; 0's will occur if word is in every single document;
#' #   tf <- DocumentTermMatrix(job_text_corpus); tf <- as.matrix(tf)
#' #   df <- as.data.frame(apply(tf,2, function(x) length(which(x > 0))))
#' #   idf <- log10(z/df)
#' #   # Get rownames of each. We need only words that are in both;
#' #   IDF_names <- rownames(idf); TF_names  <- rownames(normTF2)
#' #   # Pick the rows that have a matching word in the other dataframe
#' #   mat1 <- idf[which((IDF_names %in% TF_names) == TRUE),]
#' #   mat2 <- normTF2[which((TF_names %in% IDF_names) == TRUE),]
#' #   # Multiple them together to get tf*idf and assign rownames back
#' #   final <- as.data.frame(mat1*mat2)
#' #   rownames(final) <- NULL
#' #   rownames(final) <- TF_names[which(TF_names %in% IDF_names == TRUE)]
#' #   # Finally normalize. This is the final Normalized vector for the query.
#' #   q_final <- final/sqrt(sum(final^2))
#' #
#' #   # Now the TF of each document;
#' #   tf <- DocumentTermMatrix(job_text_corpus); tf <- as.matrix(tf)
#' #   # Weighted the TF. There is no IDF for each individual document;
#' #   tf <- t(ifelse(tf > 0, log10(tf) + 1, 0))
#' #   # Now normalize.
#' #   d_final <- apply(tf, 2,function(x) x/sqrt(sum(x^2))); #Sum(squared values) of each column = 1;
#' #
#' #   # Now we multiple the corresponding values from each query and each document.
#' #   q_names <- rownames(q_final)
#' #   d_names  <- rownames(d_final)
#' #   d_final2 <- d_final[which(d_names %in% q_names == TRUE),] #d_final, q_final should be same length
#' #   # The Final values;
#' #   prod <- as.data.frame(apply(d_final2,2,function(x) x*q_final))
#' #   for (i in 1:dim(prod)[2]){
#' #     colnames(prod)[i] <- paste0("Doc", i)
#' #   }
#' #   cos.sim <- colSums(prod)
#' #   cos.sim
#' # }


#' #' Calculates cos_sim between a resume, and job postings
#' #' @param resume_text_corpus = corpus of the resume
#' #' @param job_text_corpus = corpus of the job postings
#' #' @return the cosine similarity scores between
#' #' @export
#' #'
#' #'
#' cos_sim <- function(resume_text_corpus,job_text_corpus){
#'   # Calculate Term frequencies for jobs and the resume
#'   combined_tf <- as.matrix(TermDocumentMatrix(c(resume_text_corpus, job_text_corpus)))
#'
#'   # only keep terms that are contained in both the query (resume), and in at least one document( job posts)
#'   combined_tf <- combined_tf[combined_tf[,1] > 0,] #remove terms not in query
#'   combined_tf <- combined_tf[apply(combined_tf[,-1],1,sum) > 0,] #remove terms not in documents
#'
#'   #calculate term frequency for the query (resume)
#'   resume_tf <- combined_tf[,1]
#'   resume_log_tf <- 1 + log10(resume_tf)
#'
#'   #calculate term frequency for the document (job posts)
#'   job_tf <- combined_tf[,-1]
#'   job_log_tf <- ifelse(is.infinite(log10(job_tf)),0,1 + log10(job_tf))
#'
#'   #calculate idf
#'   doc_ocurrence <- as.matrix(apply(job_tf,1, function(x) length(which(x > 0))))
#'   idf <- log10((length(job_text_corpus)/doc_ocurrence ))
#'
#'   #calculate query (resume) tf*idf
#'   resume_tf_idf <- resume_log_tf * idf
#'
#'   #calculate document (job posts) tf*idf
#'   job_tf_idf <- apply(job_log_tf, 2, function(x) x * idf)
#'
#'   #calculate query (resume) unit vector
#'   resume_tf_idf_unit <- resume_tf_idf/sqrt(sum(resume_tf_idf^2))
#'
#'   #calculate document (job posts) tf*idf unit vector
#'   job_tf_idf_unit <- apply(job_tf_idf,2,function(x) x/sqrt(sum(x^2)))
#'
#'   #find the dot product of the two unit vectoer
#'   scores <-  apply(job_tf_idf_unit,2, function(x) crossprod(resume_tf_idf_unit, x))
#'   scores[is.na(scores)] <- 0
#'   return(scores)
#' }


